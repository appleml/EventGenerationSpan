ModelArguments(model_name_or_path='t5-base', cache_dir=None, use_fast_tokenizer=False, model_revision='main', use_auth_token=False)
DataTrainingArguments(task='event', dataset_name=None, dataset_config_name=None, text_column=None, summary_column=None, train_span_file='data/text2tree/2011_data_span/2011_train/', validation_span_file='data/text2tree/2011_data_span/2011_devel/', train_file='data/text2tree/2011_data/2011_train/', validation_file='data/text2tree/2011_data/2011_devel/', test_file='data/text2tree/2011_data/2011_test/', overwrite_cache=False, preprocessing_num_workers=None, max_source_length=256, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, max_train_samples=None, max_val_samples=None, max_test_samples=None, num_beams=None, ignore_pad_token_for_loss=True, source_prefix='event: ', decoding_format='tree', event_schema='data/text2tree/2011_data/event.schema')
ConstraintSeq2SeqTrainingArguments(output_dir='t5-base_2011bio', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=2000, logging_dir='t5-base_2011bio_log', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=421, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='t5-base_2011bio', disable_tqdm=True, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='eval_role-F1', greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True, constraint_decoding=True, label_smoothing_sum=False)
01/06/2022 15:13:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False
01/06/2022 15:13:04 - INFO - __main__ -   Training/evaluation parameters ConstraintSeq2SeqTrainingArguments(output_dir='t5-base_2011bio', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=2000, logging_dir='t5-base_2011bio_log', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=421, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='t5-base_2011bio', disable_tqdm=True, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='eval_role-F1', greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True, constraint_decoding=True, label_smoothing_sum=False)
01/06/2022 15:13:05 - WARNING - datasets.builder -   Using custom data configuration default-c4224e8b8a1ee452
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-c4224e8b8a1ee452/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]41 tables [00:00, 407.44 tables/s]229 tables [00:00, 1271.27 tables/s]382 tables [00:00, 1387.36 tables/s]573 tables [00:00, 1592.05 tables/s]765 tables [00:00, 1707.34 tables/s]                                    0 tables [00:00, ? tables/s]185 tables [00:00, 1849.06 tables/s]                                    Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c4224e8b8a1ee452/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
01/06/2022 15:13:09 - WARNING - datasets.builder -   Using custom data configuration default-b21a36b26c515ea4
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-b21a36b26c515ea4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]155 tables [00:00, 1547.17 tables/s]314 tables [00:00, 1568.17 tables/s]487 tables [00:00, 1632.65 tables/s]654 tables [00:00, 1646.98 tables/s]819 tables [00:00, 1601.99 tables/s]                                    0 tables [00:00, ? tables/s]140 tables [00:00, 1393.38 tables/s]                                    Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b21a36b26c515ea4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
loading configuration file t5-base/config.json
Model config T5Config {
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.4.2",
  "use_cache": true,
  "vocab_size": 32128
}

Didn't find file t5-base/added_tokens.json. We won't load it.
Didn't find file t5-base/special_tokens_map.json. We won't load it.
Didn't find file t5-base/tokenizer_config.json. We won't load it.
loading file t5-base/spiece.model
loading file None
loading file None
loading file None
loading file t5-base/tokenizer.json
Using bos_token, but it is not set yet.
['</s>', '<pad>']
loading weights file t5-base/pytorch_model.bin
All model checkpoint weights were used when initializing T5ForConditionalGeneration.

All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:01<00:13,  1.69s/ba] 22%|██▏       | 2/9 [00:03<00:11,  1.65s/ba] 33%|███▎      | 3/9 [00:05<00:10,  1.79s/ba] 44%|████▍     | 4/9 [00:07<00:08,  1.77s/ba] 56%|█████▌    | 5/9 [00:08<00:07,  1.84s/ba] 67%|██████▋   | 6/9 [00:10<00:05,  1.83s/ba] 78%|███████▊  | 7/9 [00:12<00:03,  1.87s/ba] 89%|████████▉ | 8/9 [00:14<00:01,  1.88s/ba]100%|██████████| 9/9 [00:15<00:00,  1.67s/ba]100%|██████████| 9/9 [00:15<00:00,  1.76s/ba]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:01<00:13,  1.68s/ba] 22%|██▏       | 2/9 [00:03<00:11,  1.67s/ba] 33%|███▎      | 3/9 [00:05<00:10,  1.80s/ba] 44%|████▍     | 4/9 [00:07<00:08,  1.77s/ba] 56%|█████▌    | 5/9 [00:08<00:07,  1.84s/ba] 67%|██████▋   | 6/9 [00:10<00:05,  1.83s/ba] 78%|███████▊  | 7/9 [00:12<00:03,  1.90s/ba] 89%|████████▉ | 8/9 [00:14<00:01,  1.89s/ba]100%|██████████| 9/9 [00:15<00:00,  1.69s/ba]100%|██████████| 9/9 [00:15<00:00,  1.77s/ba]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:01<00:03,  1.98s/ba] 67%|██████▋   | 2/3 [00:03<00:01,  1.93s/ba]100%|██████████| 3/3 [00:05<00:00,  1.87s/ba]100%|██████████| 3/3 [00:05<00:00,  1.89s/ba]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:01<00:03,  1.97s/ba] 67%|██████▋   | 2/3 [00:03<00:01,  1.92s/ba]100%|██████████| 3/3 [00:05<00:00,  1.86s/ba]100%|██████████| 3/3 [00:05<00:00,  1.88s/ba]
***** Running training *****
  Num examples = 8639
  Num Epochs = 50
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py", line 233, in run
    self._record_writer.write(data)
  File "/opt/conda/lib/python3.8/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/opt/conda/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 519, in write
    self.fs.append(self.filename, file_content, self.binary_mode)
  File "/opt/conda/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 150, in append
    self._write(filename, file_content, "ab" if binary_mode else "a")
  File "/opt/conda/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 154, in _write
    with io.open(filename, mode, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: b't5-base_2011bio_log/events.out.tfevents.1641453244.pai-101.166.0'
